# -*- coding: utf-8 -*-
"""smolvla_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10iK9DPyh6NddyzHgNzV7vflLDExxue3f

# SmolVLA Fine-tuning on Google Colab

This notebook fine-tunes the SmolVLA model (`lerobot/smolvla_base`) on your dataset using lerobot's training framework.

**Model**: [lerobot/smolvla_base](https://huggingface.co/lerobot/smolvla_base) (450M parameters)

**Dataset**: HenryZhang/Group11_data_1763075740.884942

**Reference**: [SmolVLA Blog Post](https://huggingface.co/blog/smolvla)

**Author**: JESSI11111

## Step 1: Setup Environment

First, make sure you're using a GPU runtime:
- Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU` (preferably A100 for Colab Pro)
- Then run the cells below to install dependencies
"""

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
else:
    print("‚ö†Ô∏è Warning: No GPU detected. Training will be very slow!")

"""Install the required dependencies:"""

# Commented out IPython magic to ensure Python compatibility.
# Clone LeRobot repository
!git clone https://github.com/huggingface/lerobot.git
# %cd lerobot

# Install LeRobot with SmolVLA dependencies
# %pip install -e ".[smolvla]"

# Verify installation
!python -c "import lerobot; print('LeRobot installed successfully!')"

"""## Step 2: Configuration

Configure your training parameters. Adjust these as needed:

"""

# Training configuration
# Adjust these parameters based on your needs and GPU memory

CONFIG = {
    "policy_path": "lerobot/smolvla_base",  # Pretrained model from HuggingFace
    "dataset_repo_id": "HenryZhang/Group11_data_1763075740.884942",  # Your SO101 dataset
    "batch_size": 4,  # Increased to 4 as requested (A100 can handle this)
    "steps": 20000,  # Training steps
}

print("Training Configuration:")
print("=" * 50)
for key, value in CONFIG.items():
    print(f"  {key}: {value}")
print("=" * 50)
print("\nüí° Tip: If you encounter out-of-memory (OOM) errors, reduce batch_size to 16 or 8")

"""python lerobot/scripts/train.py \
  --policy.path=lerobot/smolvla_base \
  --dataset.repo_id=HenryZhang/Group11_data_1763075740.884942 \
  --batch_size=1 \
  --steps=20000  # 10% of training budget

## Step 3: Start Training

Now we'll start the fine-tuning process using lerobot's training script. This may take a while depending on your GPU and number of steps.
"""

# Fine-tune the pretrained model
# Note: Adjust batch_size based on your GPU memory (A100 can handle 32-64)
# If you get OOM errors, reduce batch_size to 16 or 8

# Ensure we're in the lerobot directory
import os
os.chdir('/content/lerobot')

# The training script is at src/lerobot/scripts/lerobot_train.py
# Run it directly using the file path
print("Starting training...")
print(f"Current directory: {os.getcwd()}")

# Run the training script directly
!python src/lerobot/scripts/lerobot_train.py \
  --policy.type=smolvla \
  --policy.pretrained_path={CONFIG['policy_path']} \
  --policy.repo_id=smolvla_finetuned \
  --dataset.repo_id={CONFIG['dataset_repo_id']} \
  --batch_size={CONFIG['batch_size']} \
  --steps={CONFIG['steps']} \
  --optimizer.lr=5e-5 \
  --save_freq=5000 \
  --eval_freq=5000  # Evaluate every 5000 steps to monitor progress

"""It took 70 minutes for training execution

LeRobot (and most modern Hugging Face libraries) uses .safetensors. This is the modern equivalent of .pt. It stores the exact same weights but is safer and loads faster. You don't need to convert it; it works natively with LeRobot and Transformers.
"""

#push finetuned model to huggingface
import os
from huggingface_hub import HfApi
from pathlib import Path

# 1. Find the latest training output directory
output_root = Path("outputs/train")
if not output_root.exists():
    print("No output directory found. Did the training run?")
else:
    # Find all subdirectories (recursively or structured)
    # Structure seems to be outputs/train/date/time_name
    # We'll just find the most recently modified folder that contains 'checkpoints' or config
    all_dirs = [d for d in output_root.rglob("*") if d.is_dir() and "smolvla" in d.name]

    if not all_dirs:
        print("No 'smolvla' training output found.")
    else:
        # Sort by modification time, newest first
        latest_run_dir = max(all_dirs, key=os.path.getmtime)
        print(f"Found latest training run: {latest_run_dir}")

        # 2. Define your repository ID
        api = HfApi()
        try:
            user_info = api.whoami()
            username = user_info['name']
            repo_name = "smolvla_finetuned"
            repo_id = f"{username}/{repo_name}"

            print(f"Uploading to: https://huggingface.co/{repo_id}")

            # 3. Create Repo and Upload
            api.create_repo(repo_id=repo_id, exist_ok=True, repo_type="model")

            api.upload_folder(
                folder_path=str(latest_run_dir),
                repo_id=repo_id,
                repo_type="model",
                ignore_patterns=["wandb/*"]  # Ignore wandb logs if any
            )
            print("‚úÖ Upload complete! check your repo link above.")

        except Exception as e:
            print(f"‚ùå Upload failed: {e}")
            print("Make sure you ran the login cell above!")

# Other team can load the model: They just need lerobot installed.
# Example for your team to load the model directly from Hugging Face
from lerobot.common.policies.act.modeling_act import ACTPolicy

# They can load it using your Repo ID
repo_id = "JESSI11111/smolvla_finetuned"

print(f"Loading model from {repo_id}...")
policy = ACTPolicy.from_pretrained(repo_id)

print("‚úÖ Model loaded successfully! Ready for inference.")

#When you need to retrain, you can simply run pip install -r requirements.txt to restore this setup.

from google.colab import drive
import shutil
import os

# 1. Generate the requirements file from current environment
print("Generating requirements.txt...")
!pip freeze > requirements.txt

# 2. Mount Drive (checks if already mounted)
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

# 3. Define destination (using the main backup folder)
backup_dir = '/content/drive/MyDrive/lerobot_backup'
os.makedirs(backup_dir, exist_ok=True)

# 4. Copy to Drive
dest_path = os.path.join(backup_dir, 'requirements.txt')
shutil.copy('requirements.txt', dest_path)

print(f"‚úÖ Dependencies saved to: {dest_path}")
print(f"To restore later, run: !pip install -r {dest_path}")

from google.colab import drive
import shutil
import os

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Define source (your training output) and destination (your Drive)
source_dir = '/content/lerobot/outputs/train'
dest_dir = '/content/drive/MyDrive/lerobot_backup/train'

# 3. Copy files
if os.path.exists(source_dir):
    print(f"Backing up {source_dir} to {dest_dir}...")
    # Copy the directory tree
    shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)
    print("‚úÖ Backup complete! Your files are safe in Google Drive.")
else:
    print("‚ùå Source directory not found. Did you run the training?")